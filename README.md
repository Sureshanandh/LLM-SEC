# ğŸ”’ LLM-SEC â€” AI Security Scanner (v1.0)

LLM-SEC is a lightweight, cross-platform security scanner designed to test Large Language Models (LLMs) for vulnerabilities using adversarial prompts, jailbreak sequences, and safety-evasion payloads.

Inspired by tools like SQLMap, but built for LLM Offensive Security.

---

## âœ¨ Features

- ğŸš€ 900+ curated adversarial prompts  
- ğŸ”¥ Smart Auto-Detection Mode  
- ğŸ–¥ï¸ Works on Windows, Linux, macOS  
- ğŸ¨ Dynamic ANSI Banner (SQLMap-style)  
- ğŸŒ Burp Proxy support  
- ğŸ“Š Generates HTML + JSON security reports  
- ğŸ§  Detects jailbreaks, refusals, policy bypass  
- ğŸ“¡ Supports OpenAI, LM Studio, Ollama, Jan, custom API  

---

## ğŸ“¦ Installation

### 1. Clone the Repo
git clone https://github.com/YOUR_USERNAME/LLM-SEC.git

cd LLM-SEC

### 2. Install Dependencies

pip install -r requirements.txt


---

## â–¶ï¸ Usage

Run:

python llm-sec.py


Follow the on-screen prompts.

---

## ğŸ§ª Burp Suite Proxy (Optional)

Set **Use Burp? â†’ y** when starting the tool.

Traffic automatically routes through:
http://127.0.0.1:8080 (BurpSuite Proxy)


---

## ğŸ“ Output

Reports saved under:

reports/


Each run generates:

- `.json`
- `.html`
- `.txt` (console log)

---

## ğŸ“ Probes

All adversarial payloads are stored in:

probes/

You can add new ones easily.

---

## ğŸ“œ License
MIT License

---

## ğŸ¤ Contributing

Pull requests are welcome!


